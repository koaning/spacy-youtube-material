{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check out we get out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"My name is Vincent and I was born on 23rd June 1987. \\\n",
    "           I work at Rasa from Haarlem. I just bought a guitar \\\n",
    "           cost $1000 on ebay and I will get is services here for 20 euro a year.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">My name is \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Vincent\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and I was born on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    23rd June 1987\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".            I work at \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rasa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " from \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Haarlem\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". I just bought a guitar            cost $\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ebay\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and I will get is services here for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    20\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " euro a year.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Vincent, spacy.tokens.span.Span),\n",
       " (23rd June 1987, spacy.tokens.span.Span),\n",
       " (Rasa, spacy.tokens.span.Span),\n",
       " (Haarlem, spacy.tokens.span.Span),\n",
       " (1000, spacy.tokens.span.Span),\n",
       " (ebay, spacy.tokens.span.Span),\n",
       " (20, spacy.tokens.span.Span)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(e, type(e)) for e in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x1185e7780>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x11874b048>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x11874b0a8>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the spaCy Documentation\n",
    "\n",
    "![](https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x11874b0a8>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.remove_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x1185e7780>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x11874b048>)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"My name is Vincent\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing a Doc \n",
    "\n",
    "First I'll need to prepare the data such that it fits the API. Docs found [here](https://spacy.io/usage/training#training-simple-style).\n",
    "\n",
    "The docs say the data format needs to look something like this; \n",
    "\n",
    "```\n",
    "TRAIN_DATA = [\n",
    "   (\"Uber blew through $1 million\", {\"entities\": [(0, 4, \"ORG\")]}),\n",
    "   (\"Google rebrands its apps\", {\"entities\": [(0, 6, \"ORG\")]})\n",
    "]\n",
    "```\n",
    "\n",
    "So we gotta have something like;\n",
    "\n",
    "```\n",
    "TRAIN_DATA = [\n",
    "   (\"Python is cool\", {\"entities\": [(0, 6, \"PROGLANG\")]}),\n",
    "   (\"Me like golang\", {\"entities\": [(8, 14, \"PROGLANG\")]})\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Matches!\n",
    "\n",
    "I've taken the patterns code we made in the previous and put it in a seperate python file. This keeps the notebook clean and it is still easy for me to quickly get all these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import create_patterns\n",
    "matcher = Matcher(nlp.vocab, validate=True)\n",
    "matcher.add(\"PROG_LANG\", None, *create_patterns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "golang\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I do code with datastuff using python and golang.\")\n",
    "\n",
    "for idx, start, end in matcher(doc):\n",
    "    print(doc[start:end],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i like python, javascript and golang',\n",
       " {'entities': [(7, 13, 'PROGLANG'),\n",
       "   (15, 25, 'PROGLANG'),\n",
       "   (30, 36, 'PROGLANG')]})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_train_data(doc):\n",
    "    detections = [(doc[start:end].start_char, doc[start:end].end_char, 'PROGLANG') for idx, start, end in matcher(doc)]\n",
    "    return (doc.text, {'entities': detections})\n",
    "\n",
    "parse_train_data(nlp(\"i like python, javascript and golang\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training Set\n",
    "\n",
    "Now to load previous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (pd.read_csv(\"../data/have_label.txt\", \n",
    "                  nrows=5_000, \n",
    "                  sep='\\t', \n",
    "                  usecols=['Label', 'Title']))\n",
    "\n",
    "titles = df.loc[lambda d: d['Label'] == 1]['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How to set up unit testing for Visual Studio C++',\n",
       "  {'entities': [(45, 48, 'PROGLANG')]}),\n",
       " ('How do you pack a visual studio c++ project for release?',\n",
       "  {'entities': [(32, 35, 'PROGLANG')]}),\n",
       " ('How do you get leading wildcard full-text searches to work in SQL Server?',\n",
       "  {'entities': [(62, 65, 'PROGLANG')]})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA = [parse_train_data(d) for d in nlp.pipe(titles) if len(matcher(d)) == 1]\n",
    "TRAIN_DATA[5:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Again, the docs for reference are [here](https://spacy.io/usage/training#training-simple-style). We take a slightly different approach than what is listed though.\n",
    "\n",
    "We first create a blank nlp model and then add a `ner` step to it. This is easier than loading in a big model and replacing a step. It's also faster since the loading can be slow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blank_nlp(train_data):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we just run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at iteration 0 - 2020-02-29 16:01:37.407358 {'ner': 210.8715040565613}\n",
      "Losses at iteration 1 - 2020-02-29 16:01:57.611517 {'ner': 54.94050754958943}\n",
      "Losses at iteration 2 - 2020-02-29 16:02:21.511440 {'ner': 17.64956042300965}\n",
      "Losses at iteration 3 - 2020-02-29 16:02:46.515510 {'ner': 22.03818076914257}\n",
      "Losses at iteration 4 - 2020-02-29 16:03:12.728737 {'ner': 32.78210120097184}\n",
      "Losses at iteration 5 - 2020-02-29 16:03:37.277097 {'ner': 36.18430367359715}\n",
      "Losses at iteration 6 - 2020-02-29 16:04:02.517578 {'ner': 12.249202834523112}\n",
      "Losses at iteration 7 - 2020-02-29 16:04:27.960499 {'ner': 0.0001372906562084279}\n",
      "Losses at iteration 8 - 2020-02-29 16:04:53.461668 {'ner': 1.167531842838531e-09}\n",
      "Losses at iteration 9 - 2020-02-29 16:05:18.533350 {'ner': 6.024408435729552e-10}\n",
      "Losses at iteration 10 - 2020-02-29 16:05:44.401320 {'ner': 3.3534985043498425e-08}\n",
      "Losses at iteration 11 - 2020-02-29 16:06:09.925620 {'ner': 1.124431198414007e-09}\n",
      "Losses at iteration 12 - 2020-02-29 16:06:35.726901 {'ner': 4.196353389998916e-10}\n",
      "Losses at iteration 13 - 2020-02-29 16:07:00.113206 {'ner': 1.1748750115589748e-09}\n",
      "Losses at iteration 14 - 2020-02-29 16:07:27.353994 {'ner': 8.740565113203031}\n",
      "Losses at iteration 15 - 2020-02-29 16:07:54.558443 {'ner': 64.52540968504937}\n",
      "Losses at iteration 16 - 2020-02-29 16:08:20.530810 {'ner': 12.676441719872146}\n",
      "Losses at iteration 17 - 2020-02-29 16:08:53.355646 {'ner': 4.788162612750786}\n",
      "Losses at iteration 18 - 2020-02-29 16:09:23.806350 {'ner': 0.00021747206463490674}\n",
      "Losses at iteration 19 - 2020-02-29 16:09:53.547554 {'ner': 3.142495195321863}\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import datetime as dt\n",
    "\n",
    "nlp = create_blank_nlp(TRAIN_DATA)\n",
    "optimizer = nlp.begin_training()  \n",
    "for i in range(20):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        nlp.update([text], [annotations], sgd=optimizer, losses=losses)\n",
    "    print(f\"Losses at iteration {i} - {dt.datetime.now()}\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements \n",
    "\n",
    "I'll just add some things that makes training these things slightly nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at iteration 0 - 2020-02-29 16:13:35.823776 {'ner': 421.81081383064986}\n",
      "Losses at iteration 1 - 2020-02-29 16:13:40.236429 {'ner': 16.171604070858784}\n",
      "Losses at iteration 2 - 2020-02-29 16:13:45.031095 {'ner': 10.869232156674228}\n",
      "Losses at iteration 3 - 2020-02-29 16:13:50.309758 {'ner': 5.347369765463781}\n",
      "Losses at iteration 4 - 2020-02-29 16:13:54.814064 {'ner': 5.267283654703734}\n",
      "Losses at iteration 5 - 2020-02-29 16:13:59.583930 {'ner': 7.034331411273773}\n",
      "Losses at iteration 6 - 2020-02-29 16:14:04.977785 {'ner': 20.55244086534093}\n",
      "Losses at iteration 7 - 2020-02-29 16:14:11.207178 {'ner': 16.854737952514622}\n",
      "Losses at iteration 8 - 2020-02-29 16:14:16.702827 {'ner': 12.846826920458023}\n",
      "Losses at iteration 9 - 2020-02-29 16:14:22.886344 {'ner': 7.316021861073125}\n",
      "Losses at iteration 10 - 2020-02-29 16:14:29.519257 {'ner': 0.20566945497729483}\n",
      "Losses at iteration 11 - 2020-02-29 16:14:36.143884 {'ner': 3.7788202090958585}\n",
      "Losses at iteration 12 - 2020-02-29 16:14:42.415683 {'ner': 7.465032841846728}\n",
      "Losses at iteration 13 - 2020-02-29 16:14:48.875113 {'ner': 2.3451352358449675e-07}\n",
      "Losses at iteration 14 - 2020-02-29 16:14:54.256928 {'ner': 3.7464054022410176e-07}\n",
      "Losses at iteration 15 - 2020-02-29 16:15:00.729210 {'ner': 1.5179423036044701e-05}\n",
      "Losses at iteration 16 - 2020-02-29 16:15:06.206157 {'ner': 5.0365825329424435e-05}\n",
      "Losses at iteration 17 - 2020-02-29 16:15:12.115493 {'ner': 9.695175597872534e-09}\n",
      "Losses at iteration 18 - 2020-02-29 16:15:18.315275 {'ner': 1.2864200256492409e-08}\n",
      "Losses at iteration 19 - 2020-02-29 16:15:24.544488 {'ner': 2.5533360552627274e-08}\n"
     ]
    }
   ],
   "source": [
    "nlp = create_blank_nlp(TRAIN_DATA)\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(20):\n",
    "    losses = {}\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "            texts,  # batch of texts\n",
    "            annotations,  # batch of annotations\n",
    "            drop=0.1,  # dropout - make it harder to memorise data\n",
    "            losses=losses,\n",
    "        )\n",
    "    print(f\"Losses at iteration {i} - {dt.datetime.now()} {losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x13cabd2e8>)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"i write code in python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">i write code in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    python\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PROGLANG</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"i write code in python\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">i write code in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    python\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PROGLANG</span>\n",
       "</mark>\n",
       " and go</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"i write code in python and go\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
